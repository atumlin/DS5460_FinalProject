{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS5460 Milestone 2 - EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting Files to PySpark\n",
    "\n",
    "Author: Anne Tumlin\n",
    "\n",
    "Date: 03/21/25\n",
    "\n",
    "Now that we have taken the files from the original GCS bucket, extracted them, and put them in our local GCS bucket (see `docs/EXTRACTION_PROCESS` in GitHub for more details), we can begin to ingest our data into PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"app_name\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to edit the path with YOUR google storage bucket here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/*.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on a Small Scale\n",
    "Let's try this with only 100 files first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: Change bucket_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"ds5460-tumlinam-fp-bucket\"\n",
    "bucket = client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List blobs (files) in the specified prefix and collect the first 100 file paths\n",
    "blobs = bucket.list_blobs(prefix=prefix)\n",
    "file_paths = []\n",
    "for blob in blobs:\n",
    "    file_paths.append(f\"gs://{bucket_name}/{blob.name}\")\n",
    "    if len(file_paths) >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the following 100 JSON file paths:\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15000.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15001.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15002.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15003.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15004.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15005.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15006.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15007.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15008.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15009.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15010.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15011.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15012.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15013.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15014.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15015.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15016.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15017.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15018.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15019.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15020.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15021.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15022.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15023.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15024.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15025.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15026.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15027.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15028.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15029.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15030.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15031.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15032.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15033.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15034.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15035.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15036.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15037.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15038.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15039.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15040.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15041.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15042.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15043.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15044.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15045.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15046.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15047.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15048.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15049.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15050.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15051.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15052.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15053.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15054.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15055.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15056.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15057.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15058.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15059.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15060.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15061.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15062.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15063.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15064.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15065.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15066.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15067.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15068.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15069.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15070.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15071.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15072.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15073.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15074.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15075.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15076.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15077.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15078.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15079.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15080.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15081.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15082.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15083.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15084.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15085.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15086.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15087.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15088.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15089.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15090.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15091.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15092.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15093.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15094.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15095.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15096.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15097.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15098.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15099.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the following 100 JSON file paths:\")\n",
    "for path in file_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** After testing and running into issues with the schema, I discovered that due to the way the JSON files are formatted we must utilize the multiline read option. Otherwise, our data will not be read in properly. Instead, it will lead to the error `|-- _corrupt_record: string (nullable = true)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use multiline read option!\n",
    "df_small = spark.read.option(\"multiline\", \"true\").json(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grid: struct (nullable = true)\n",
      " |    |-- context: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |-- edges: struct (nullable = true)\n",
      " |    |    |-- ac_line: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- generator_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- load_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- shunt_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- transformer: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |-- nodes: struct (nullable = true)\n",
      " |    |    |-- bus: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- generator: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- load: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- shunt: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- objective: double (nullable = true)\n",
      " |-- solution: struct (nullable = true)\n",
      " |    |-- edges: struct (nullable = true)\n",
      " |    |    |-- ac_line: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- transformer: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |-- nodes: struct (nullable = true)\n",
      " |    |    |-- bus: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- generator: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                grid|            metadata|            solution|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[[[[100.0]]], [[[...| [443934.8106702195]|[[[[[1.2271252469...|\n",
      "|[[[[100.0]]], [[[...|[465533.45792886155]|[[[[[1.3700529900...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiline\", \"true\").json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Summarizing Relevant Datasets Code for the New Dataset\n",
    "  \n",
    "Author: Ewan Long\n",
    "  \n",
    "Date: 03/23/25\n",
    "  \n",
    "We are trying to test and revise the original code we have in the Milestone 1 as we changed our dataset (see `docs/EXTRACTION_PROCESS` in GitHub for reasons & more details), we have ingested our data into PySpark. \n",
    "  \n",
    "The following code will try to aggregate data and join the key tables (see Milestone 1 Notebook and doc for reasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, initialize the SparkSession. We will process the first 100 JSON files here for faster processing and test verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, explode, input_file_name, expr, sum as spark_sum, avg, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/24 04:05:44 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/24 04:05:44 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/24 04:05:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/24 04:05:44 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"app_name\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_path = \"gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/*.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change the bucket to your own's\n",
    "bucket_name = \"dataproc-temp-us-central1-299400297029-n5lhuci2\"\n",
    "bucket = client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List blobs (files) in the specified prefix and collect the first 100 file paths\n",
    "blobs = bucket.list_blobs(prefix=prefix)\n",
    "file_paths = []\n",
    "for blob in blobs:\n",
    "    file_paths.append(f\"gs://{bucket_name}/{blob.name}\")\n",
    "    if len(file_paths) >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the following 100 JSON file paths:\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15000.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15001.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15002.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15003.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15004.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15005.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15006.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15007.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15008.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15009.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15010.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15011.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15012.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15013.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15014.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15015.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15016.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15017.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15018.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15019.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15020.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15021.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15022.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15023.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15024.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15025.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15026.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15027.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15028.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15029.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15030.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15031.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15032.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15033.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15034.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15035.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15036.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15037.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15038.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15039.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15040.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15041.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15042.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15043.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15044.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15045.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15046.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15047.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15048.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15049.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15050.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15051.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15052.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15053.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15054.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15055.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15056.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15057.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15058.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15059.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15060.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15061.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15062.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15063.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15064.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15065.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15066.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15067.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15068.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15069.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15070.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15071.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15072.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15073.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15074.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15075.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15076.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15077.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15078.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15079.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15080.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15081.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15082.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15083.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15084.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15085.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15086.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15087.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15088.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15089.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15090.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15091.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15092.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15093.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15094.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15095.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15096.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15097.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15098.json\n",
      "gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15099.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the following 100 JSON file paths:\")\n",
    "for path in file_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, read all JSON files with the multiline option enabled so that nested JSON is parsed correctly. Adding the original **filename** column for identification and join purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grid: struct (nullable = true)\n",
      " |    |-- context: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |-- edges: struct (nullable = true)\n",
      " |    |    |-- ac_line: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- generator_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- load_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- shunt_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- transformer: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |-- nodes: struct (nullable = true)\n",
      " |    |    |-- bus: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- generator: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- load: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- shunt: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- objective: double (nullable = true)\n",
      " |-- solution: struct (nullable = true)\n",
      " |    |-- edges: struct (nullable = true)\n",
      " |    |    |-- ac_line: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- transformer: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |-- nodes: struct (nullable = true)\n",
      " |    |    |-- bus: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- generator: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |-- filename: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.option(\"multiline\", \"true\").json(file_paths).withColumn(\"filename\", input_file_name())\n",
    "\n",
    "# Print the schema to verify that 'grid.nodes.generator' and others are parsed correctly.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can do the data processing. Let's process generator data first. \n",
    "  \n",
    "I will explode nested JSON and extract generator attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+--------------------+-----+--------+--------+-------+---+------------+-----------+-----------+\n",
      "|            filename|mbase|                 pg|                pmin| pmax|      qg|    qmin|   qmax| vg|cost_squared|cost_linear|cost_offset|\n",
      "+--------------------+-----+-------------------+--------------------+-----+--------+--------+-------+---+------------+-----------+-----------+\n",
      "|gs://dataproc-tem...| 46.9|            0.34463|             0.22026|0.469| 0.07856|-0.02298| 0.1801|1.0|         0.0|     3000.0|        0.0|\n",
      "|gs://dataproc-tem...|888.2|            6.43217|  3.9823399999999998|8.882| 1.27013|-0.72832|3.26858|1.0|       10.98|      910.9|        0.0|\n",
      "|gs://dataproc-tem...| 25.0|            0.16691| 0.08381999999999999| 0.25|0.041875|-0.01225|  0.096|1.0|         0.0|     3000.0|        0.0|\n",
      "|gs://dataproc-tem...| 25.0|            0.13858|0.027160000000000004| 0.25|0.041875|-0.01225|  0.096|1.0|         0.0|     3000.0|        0.0|\n",
      "|gs://dataproc-tem...| 25.0|0.17330500000000001|             0.09661| 0.25|0.041875|-0.01225|  0.096|1.0|         0.0|     3000.0|        0.0|\n",
      "+--------------------+-----+-------------------+--------------------+-----+--------+--------+-------+---+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_df = df.select(\n",
    "    \"filename\",\n",
    "    explode(\"grid.nodes.generator\").alias(\"generator_array\")\n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"generator_array\")[0].alias(\"mbase\"),\n",
    "    col(\"generator_array\")[1].alias(\"pg\"),\n",
    "    col(\"generator_array\")[2].alias(\"pmin\"),\n",
    "    col(\"generator_array\")[3].alias(\"pmax\"),\n",
    "    col(\"generator_array\")[4].alias(\"qg\"),\n",
    "    col(\"generator_array\")[5].alias(\"qmin\"),\n",
    "    col(\"generator_array\")[6].alias(\"qmax\"),\n",
    "    col(\"generator_array\")[7].alias(\"vg\"),\n",
    "    col(\"generator_array\")[8].alias(\"cost_squared\"),\n",
    "    col(\"generator_array\")[9].alias(\"cost_linear\"),\n",
    "    col(\"generator_array\")[10].alias(\"cost_offset\")\n",
    ")\n",
    "\n",
    "gen_df.show(5) # verify the extracted attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to aggregates generator features at the snapshort level, summarizing key characteristics for each filename.\n",
    "  \n",
    "The following statistics will be extracted and computed:\n",
    "- Total number of generators\n",
    "- Sum of generated power (pg)\n",
    "- Average generator voltage (vg)\n",
    "- Average generator cost parameters (quadratic, linear, and offset terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate generator features by snapshot (filename)\n",
    "gen_agg = gen_df.groupBy(\"filename\").agg(\n",
    "    count(\"*\").alias(\"num_generators\"),\n",
    "    spark_sum(\"pg\").alias(\"total_pg\"),\n",
    "    avg(\"vg\").alias(\"avg_vg\"),\n",
    "    avg(\"cost_squared\").alias(\"avg_cost_squared\"),\n",
    "    avg(\"cost_linear\").alias(\"avg_cost_linear\"),\n",
    "    avg(\"cost_offset\").alias(\"avg_cost_offset\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would try to process and aggregate the power load data, extracting key load parameters (specifically real \"pd\" and reactive \"qd\" power demands) per snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                         (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_27463/2439983976.py\", line 14, in <module>\n",
      "    load_df.show(5)\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 947, in show\n",
      "    print(self._show_string(n, truncate, vertical))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 965, in _show_string\n",
      "    return self._jdf.showString(n, 20, vertical)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m load_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid.nodes.load\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_arrays\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[43mload_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03mname | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2181\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2178\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2181\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2183\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Extracting load data\n",
    "load_df = df.select(\n",
    "    \"filename\",\n",
    "    col(\"grid.nodes.load\").alias(\"load_arrays\")  \n",
    ").select(\n",
    "    \"filename\",\n",
    "    explode(\"load_arrays\").alias(\"load\")  \n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"load\")[0].alias(\"pd\"),  \n",
    "    col(\"load\")[1].alias(\"qd\")\n",
    ")\n",
    "\n",
    "load_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate load features per snapshot\n",
    "load_agg = load_df.groupBy(\"filename\").agg(\n",
    "    count(\"*\").alias(\"num_loads\"),\n",
    "    spark_sum(\"pd\").alias(\"total_pd\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the next step would try to extract and aggregate transmission line features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract transmission line features from grid.edges.ac_line.features\n",
    "ac_line_df = df.select(\n",
    "    \"filename\",\n",
    "    explode(\"grid.edges.ac_line.features\").alias(\"ac_line\")\n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"ac_line\")[2].alias(\"br_r\"),\n",
    "    col(\"ac_line\")[3].alias(\"br_x\"),\n",
    "    col(\"ac_line\")[4].alias(\"rate_a\"),\n",
    "    col(\"ac_line\")[5].alias(\"rate_b\"),\n",
    "    col(\"ac_line\")[6].alias(\"rate_c\")\n",
    ")\n",
    "\n",
    "ac_line_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate transformer features per snapshot\n",
    "ac_line_agg = ac_line_df.groupBy(\"filename\").agg(\n",
    "    avg(\"br_r\").alias(\"br_r_mean\"),\n",
    "    avg(\"br_x\").alias(\"br_x_mean\"),\n",
    "    spark_sum(\"rate_a\").alias(\"rate_a_sum\"),\n",
    "    F.min(\"rate_b\").alias(\"rate_b_min\"),\n",
    "    F.max(\"rate_c\").alias(\"rate_c_max\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, extract and aggregate the Transformer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract transformer features from grid.edges.transformer.features\n",
    "trans_df = df.select(\n",
    "    \"filename\",\n",
    "    explode(\"grid.edges.transformer.features\").alias(\"trans\")\n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"trans\")[2].alias(\"br_r\"),\n",
    "    col(\"trans\")[3].alias(\"br_x\"),\n",
    "    col(\"trans\")[4].alias(\"rate_a\"),\n",
    "    col(\"trans\")[7].alias(\"tap\")\n",
    ")\n",
    "\n",
    "trans_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate transformer features per snapshot\n",
    "trans_agg = trans_df.groupBy(\"filename\").agg(\n",
    "    avg(\"br_r\").alias(\"trans_br_r_mean\"),\n",
    "    avg(\"br_x\").alias(\"trans_br_x_mean\"),\n",
    "    spark_sum(\"rate_a\").alias(\"trans_rate_a_sum\"),\n",
    "    avg(\"tap\").alias(\"tap_mean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, extract the objetive value (total cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the objective value directly from metadata\n",
    "obj_df = df.select(\n",
    "    \"filename\",\n",
    "    col(\"metadata.`objective`\").alias(\"total_cost\") \n",
    ")\n",
    "\n",
    "\n",
    "obj_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will merge aggregated data from different sources and ensures data consistency by checking for duplicate entries. Here are the main steps:\n",
    "  \n",
    "- Joining aggregated data from generator, load, transmission line, transformer and objective value based on \"filename\" into a single DataFrame.\n",
    "- Validating the aggregated results\n",
    "- Checking for duplicate filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use inner joins on 'filename' (the snapshot key) to combine all real values\n",
    "agg_df = gen_agg.join(load_agg, \"filename\", \"inner\") \\\n",
    "                .join(ac_line_agg, \"filename\", \"inner\") \\\n",
    "                .join(trans_agg, \"filename\", \"inner\") \\\n",
    "                .join(obj_df, \"filename\", \"inner\")\n",
    "\n",
    "# Validate the result\n",
    "print(\"Total number of rows after aggregation:\", agg_df.count())\n",
    "agg_df.show(5, truncate=False)  # Display the first 5 rows without truncation\n",
    "\n",
    "# Check for duplicate filenames\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "duplicate_check = agg_df.groupBy(\"filename\").agg(\n",
    "    F.count(\"*\").alias(\"count\")\n",
    ").filter(F.col(\"count\") > 1)\n",
    "\n",
    "if duplicate_check.count() == 0:\n",
    "    print(\"All filenames are unique\")\n",
    "else:\n",
    "    print(\"Duplicate filenames exist! Data consistency needs to be checked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
