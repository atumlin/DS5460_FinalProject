{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS5460 Milestone 2 - EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting Files to PySpark\n",
    "\n",
    "Author: Anne Tumlin\n",
    "\n",
    "Date: 03/21/25\n",
    "\n",
    "Now that we have taken the files from the original GCS bucket, extracted them, and put them in our local GCS bucket (see `docs/EXTRACTION_PROCESS` in GitHub for more details), we can begin to ingest our data into PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"app_name\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to edit the path with YOUR google storage bucket here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/*.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on a Small Scale\n",
    "Let's try this with only 100 files first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: Change bucket_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"ds5460-tumlinam-fp-bucket\"\n",
    "bucket = client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List blobs (files) in the specified prefix and collect the first 100 file paths\n",
    "blobs = bucket.list_blobs(prefix=prefix)\n",
    "file_paths = []\n",
    "for blob in blobs:\n",
    "    file_paths.append(f\"gs://{bucket_name}/{blob.name}\")\n",
    "    if len(file_paths) >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the following 100 JSON file paths:\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15000.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15001.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15002.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15003.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15004.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15005.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15006.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15007.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15008.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15009.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15010.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15011.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15012.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15013.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15014.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15015.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15016.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15017.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15018.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15019.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15020.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15021.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15022.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15023.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15024.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15025.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15026.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15027.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15028.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15029.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15030.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15031.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15032.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15033.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15034.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15035.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15036.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15037.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15038.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15039.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15040.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15041.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15042.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15043.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15044.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15045.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15046.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15047.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15048.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15049.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15050.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15051.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15052.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15053.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15054.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15055.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15056.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15057.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15058.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15059.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15060.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15061.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15062.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15063.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15064.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15065.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15066.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15067.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15068.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15069.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15070.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15071.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15072.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15073.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15074.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15075.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15076.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15077.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15078.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15079.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15080.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15081.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15082.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15083.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15084.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15085.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15086.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15087.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15088.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15089.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15090.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15091.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15092.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15093.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15094.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15095.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15096.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15097.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15098.json\n",
      "gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15099.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the following 100 JSON file paths:\")\n",
    "for path in file_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** After testing and running into issues with the schema, I discovered that due to the way the JSON files are formatted we must utilize the multiline read option. Otherwise, our data will not be read in properly. Instead, it will lead to the error `|-- _corrupt_record: string (nullable = true)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use multiline read option!\n",
    "df_small = spark.read.option(\"multiline\", \"true\").json(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grid: struct (nullable = true)\n",
      " |    |-- context: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |-- edges: struct (nullable = true)\n",
      " |    |    |-- ac_line: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- generator_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- load_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- shunt_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- transformer: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |-- nodes: struct (nullable = true)\n",
      " |    |    |-- bus: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- generator: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- load: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- shunt: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- objective: double (nullable = true)\n",
      " |-- solution: struct (nullable = true)\n",
      " |    |-- edges: struct (nullable = true)\n",
      " |    |    |-- ac_line: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- transformer: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |-- nodes: struct (nullable = true)\n",
      " |    |    |-- bus: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- generator: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                grid|            metadata|            solution|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[[[[100.0]]], [[[...| [443934.8106702195]|[[[[[1.2271252469...|\n",
      "|[[[[100.0]]], [[[...|[465533.45792886155]|[[[[[1.3700529900...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiline\", \"true\").json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Summarizing Relevant Datasets Code for the New Dataset\n",
    "  \n",
    "Author: Ewan Long\n",
    "  \n",
    "Date: 03/23/25\n",
    "  \n",
    "We are trying to test and revise the original code we have in the Milestone 1 as we changed our dataset (see `docs/EXTRACTION_PROCESS` in GitHub for reasons & more details), we have ingested our data into PySpark. \n",
    "  \n",
    "The following code will try to aggregate data and join the key tables (see Milestone 1 Notebook and doc for reasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, initialize the SparkSession. We will process the first 100 JSON files here for faster processing and test verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, explode, input_file_name, expr, sum as spark_sum, avg, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"app_name\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"gs://dataproc-temp-us-central1-299400297029-n5lhuci2/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change the bucket to your own's\n",
    "bucket_name = \"bucketseattledonna\"\n",
    "bucket = client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List blobs (files) in the specified prefix and collect the first 100 file paths\n",
    "blobs = bucket.list_blobs(prefix=prefix)\n",
    "file_paths = []\n",
    "for blob in blobs:\n",
    "    file_paths.append(f\"gs://{bucket_name}/{blob.name}\")\n",
    "    if len(file_paths) >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the following 100 JSON file paths:\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15000.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15001.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15002.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15003.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15004.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15005.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15006.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15007.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15008.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15009.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15010.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15011.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15012.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15013.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15014.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15015.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15016.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15017.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15018.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15019.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15020.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15021.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15022.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15023.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15024.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15025.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15026.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15027.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15028.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15029.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15030.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15031.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15032.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15033.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15034.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15035.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15036.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15037.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15038.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15039.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15040.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15041.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15042.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15043.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15044.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15045.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15046.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15047.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15048.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15049.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15050.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15051.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15052.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15053.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15054.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15055.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15056.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15057.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15058.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15059.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15060.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15061.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15062.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15063.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15064.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15065.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15066.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15067.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15068.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15069.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15070.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15071.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15072.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15073.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15074.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15075.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15076.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15077.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15078.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15079.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15080.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15081.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15082.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15083.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15084.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15085.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15086.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15087.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15088.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15089.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15090.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15091.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15092.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15093.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15094.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15095.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15096.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15097.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15098.json\n",
      "gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15099.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the following 100 JSON file paths:\")\n",
    "for path in file_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, read all JSON files with the multiline option enabled so that nested JSON is parsed correctly. Adding the original **filename** column for identification and join purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grid: struct (nullable = true)\n",
      " |    |-- context: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |-- edges: struct (nullable = true)\n",
      " |    |    |-- ac_line: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- generator_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- load_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- shunt_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- transformer: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |-- nodes: struct (nullable = true)\n",
      " |    |    |-- bus: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- generator: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- load: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- shunt: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- objective: double (nullable = true)\n",
      " |-- solution: struct (nullable = true)\n",
      " |    |-- edges: struct (nullable = true)\n",
      " |    |    |-- ac_line: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- transformer: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |-- nodes: struct (nullable = true)\n",
      " |    |    |-- bus: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- generator: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |-- filename: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.option(\"multiline\", \"true\").json(file_paths).withColumn(\"filename\", input_file_name())\n",
    "\n",
    "# Print the schema to verify that 'grid.nodes.generator' and others are parsed correctly.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can do the data processing. Let's process generator data first. \n",
    "  \n",
    "I will explode nested JSON and extract generator attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+--------------------+-----+--------+--------+-------+---+------------+-----------+-----------+\n",
      "|            filename|mbase|                 pg|                pmin| pmax|      qg|    qmin|   qmax| vg|cost_squared|cost_linear|cost_offset|\n",
      "+--------------------+-----+-------------------+--------------------+-----+--------+--------+-------+---+------------+-----------+-----------+\n",
      "|gs://bucketseattl...| 46.9|            0.34463|             0.22026|0.469| 0.07856|-0.02298| 0.1801|1.0|         0.0|     3000.0|        0.0|\n",
      "|gs://bucketseattl...|888.2|            6.43217|  3.9823399999999998|8.882| 1.27013|-0.72832|3.26858|1.0|       10.98|      910.9|        0.0|\n",
      "|gs://bucketseattl...| 25.0|            0.16691| 0.08381999999999999| 0.25|0.041875|-0.01225|  0.096|1.0|         0.0|     3000.0|        0.0|\n",
      "|gs://bucketseattl...| 25.0|            0.13858|0.027160000000000004| 0.25|0.041875|-0.01225|  0.096|1.0|         0.0|     3000.0|        0.0|\n",
      "|gs://bucketseattl...| 25.0|0.17330500000000001|             0.09661| 0.25|0.041875|-0.01225|  0.096|1.0|         0.0|     3000.0|        0.0|\n",
      "+--------------------+-----+-------------------+--------------------+-----+--------+--------+-------+---+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_df = df.select(\n",
    "    \"filename\",\n",
    "    explode(\"grid.nodes.generator\").alias(\"generator_array\")\n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"generator_array\")[0].alias(\"mbase\"),\n",
    "    col(\"generator_array\")[1].alias(\"pg\"),\n",
    "    col(\"generator_array\")[2].alias(\"pmin\"),\n",
    "    col(\"generator_array\")[3].alias(\"pmax\"),\n",
    "    col(\"generator_array\")[4].alias(\"qg\"),\n",
    "    col(\"generator_array\")[5].alias(\"qmin\"),\n",
    "    col(\"generator_array\")[6].alias(\"qmax\"),\n",
    "    col(\"generator_array\")[7].alias(\"vg\"),\n",
    "    col(\"generator_array\")[8].alias(\"cost_squared\"),\n",
    "    col(\"generator_array\")[9].alias(\"cost_linear\"),\n",
    "    col(\"generator_array\")[10].alias(\"cost_offset\")\n",
    ")\n",
    "\n",
    "gen_df.show(5) # verify the extracted attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to aggregates generator features at the snapshort level, summarizing key characteristics for each filename.\n",
    "  \n",
    "The following statistics will be extracted and computed:\n",
    "- Total number of generators\n",
    "- Sum of generated power (pg)\n",
    "- Average generator voltage (vg)\n",
    "- Average generator cost parameters (quadratic, linear, and offset terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate generator features by snapshot (filename)\n",
    "gen_agg = gen_df.groupBy(\"filename\").agg(\n",
    "    count(\"*\").alias(\"num_generators\"),\n",
    "    spark_sum(\"pg\").alias(\"total_pg\"),\n",
    "    avg(\"vg\").alias(\"avg_vg\"),\n",
    "    avg(\"cost_squared\").alias(\"avg_cost_squared\"),\n",
    "    avg(\"cost_linear\").alias(\"avg_cost_linear\"),\n",
    "    avg(\"cost_offset\").alias(\"avg_cost_offset\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would try to process and aggregate the power load data, extracting key load parameters (specifically real \"pd\" and reactive \"qd\" power demands) per snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+\n",
      "|            filename|                 pd|                  qd|\n",
      "+--------------------+-------------------+--------------------+\n",
      "|gs://bucketseattl...|0.32349711373191603|  0.0541399530109636|\n",
      "|gs://bucketseattl...| 0.8572351780984674|   0.253666451943297|\n",
      "|gs://bucketseattl...| 0.5207840155565728| 0.12156684298552808|\n",
      "|gs://bucketseattl...| 0.7133438352218501|0.039086170526506626|\n",
      "|gs://bucketseattl...| 0.6837468540898303| 0.13888321084001698|\n",
      "+--------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting load data\n",
    "load_df = df.select(\n",
    "    \"filename\",\n",
    "    col(\"grid.nodes.load\").alias(\"load_arrays\")  \n",
    ").select(\n",
    "    \"filename\",\n",
    "    explode(\"load_arrays\").alias(\"load\")  \n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"load\")[0].alias(\"pd\"),  \n",
    "    col(\"load\")[1].alias(\"qd\")\n",
    ")\n",
    "\n",
    "load_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate load features per snapshot\n",
    "load_agg = load_df.groupBy(\"filename\").agg(\n",
    "    count(\"*\").alias(\"num_loads\"),\n",
    "    spark_sum(\"pd\").alias(\"total_pd\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the next step would try to extract and aggregate transmission line features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+---------+------------------+\n",
      "|            filename|      br_r|      br_x|    rate_a|   rate_b|            rate_c|\n",
      "+--------------------+----------+----------+----------+---------+------------------+\n",
      "|gs://bucketseattl...|0.01340085|0.01340085| 0.0154525|0.0792528|            2.3994|\n",
      "|gs://bucketseattl...| 0.0065814| 0.0065814|0.00358046|0.0258456|            2.6003|\n",
      "|gs://bucketseattl...| 0.0065814| 0.0065814|0.00358046|0.0258456|            2.6003|\n",
      "|gs://bucketseattl...| 0.0062579| 0.0062579|0.00319977|0.0219502|2.5658999999999996|\n",
      "|gs://bucketseattl...|0.00557485|0.00557485|0.00696197|0.0360534|            2.1877|\n",
      "+--------------------+----------+----------+----------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract transmission line features from grid.edges.ac_line.features\n",
    "ac_line_df = df.select(\n",
    "    \"filename\",\n",
    "    explode(\"grid.edges.ac_line.features\").alias(\"ac_line\")\n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"ac_line\")[2].alias(\"br_r\"),\n",
    "    col(\"ac_line\")[3].alias(\"br_x\"),\n",
    "    col(\"ac_line\")[4].alias(\"rate_a\"),\n",
    "    col(\"ac_line\")[5].alias(\"rate_b\"),\n",
    "    col(\"ac_line\")[6].alias(\"rate_c\")\n",
    ")\n",
    "\n",
    "ac_line_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate transformer features per snapshot\n",
    "ac_line_agg = ac_line_df.groupBy(\"filename\").agg(\n",
    "    avg(\"br_r\").alias(\"br_r_mean\"),\n",
    "    avg(\"br_x\").alias(\"br_x_mean\"),\n",
    "    spark_sum(\"rate_a\").alias(\"rate_a_sum\"),\n",
    "    F.min(\"rate_b\").alias(\"rate_b_min\"),\n",
    "    F.max(\"rate_c\").alias(\"rate_c_max\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, extract and aggregate the Transformer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+------+-------+\n",
      "|            filename|      br_r|     br_x|rate_a|    tap|\n",
      "+--------------------+----------+---------+------+-------+\n",
      "|gs://bucketseattl...|5.36254E-4|0.0293108| 3.905| 1.0125|\n",
      "|gs://bucketseattl...|5.05881E-4|0.0243569|5.0692|1.03125|\n",
      "|gs://bucketseattl...|5.41271E-4|0.0336934|4.6108|    1.1|\n",
      "|gs://bucketseattl...|5.41271E-4|0.0336934|4.6108|    1.1|\n",
      "|gs://bucketseattl...|5.41271E-4|0.0336934|4.6108|    1.1|\n",
      "+--------------------+----------+---------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract transformer features from grid.edges.transformer.features\n",
    "trans_df = df.select(\n",
    "    \"filename\",\n",
    "    explode(\"grid.edges.transformer.features\").alias(\"trans\")\n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"trans\")[2].alias(\"br_r\"),\n",
    "    col(\"trans\")[3].alias(\"br_x\"),\n",
    "    col(\"trans\")[4].alias(\"rate_a\"),\n",
    "    col(\"trans\")[7].alias(\"tap\")\n",
    ")\n",
    "\n",
    "trans_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate transformer features per snapshot\n",
    "trans_agg = trans_df.groupBy(\"filename\").agg(\n",
    "    avg(\"br_r\").alias(\"trans_br_r_mean\"),\n",
    "    avg(\"br_x\").alias(\"trans_br_x_mean\"),\n",
    "    spark_sum(\"rate_a\").alias(\"trans_rate_a_sum\"),\n",
    "    avg(\"tap\").alias(\"tap_mean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, extract the objetive value (total cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            filename|        total_cost|\n",
      "+--------------------+------------------+\n",
      "|gs://bucketseattl...| 443934.8106702195|\n",
      "|gs://bucketseattl...|465533.45792886155|\n",
      "|gs://bucketseattl...| 462092.4328079719|\n",
      "|gs://bucketseattl...| 453110.9782805654|\n",
      "|gs://bucketseattl...|452267.02218503354|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the objective value directly from metadata\n",
    "obj_df = df.select(\n",
    "    \"filename\",\n",
    "    col(\"metadata.`objective`\").alias(\"total_cost\") \n",
    ")\n",
    "\n",
    "\n",
    "obj_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will merge aggregated data from different sources and ensures data consistency by checking for duplicate entries. Here are the main steps:\n",
    "  \n",
    "- Joining aggregated data from generator, load, transmission line, transformer and objective value based on \"filename\" into a single DataFrame.\n",
    "- Validating the aggregated results\n",
    "- Checking for duplicate filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows after aggregation: 100\n",
      "+--------------------------------------------------------------------------------------------------------------+--------------+------------------+------+------------------+-----------------+------------------+---------+------------------+--------------------+--------------------+------------------+----------+----------+---------------------+-------------------+-----------------+------------------+------------------+\n",
      "|filename                                                                                                      |num_generators|total_pg          |avg_vg|avg_cost_squared  |avg_cost_linear  |avg_cost_offset   |num_loads|total_pd          |br_r_mean           |br_x_mean           |rate_a_sum        |rate_b_min|rate_c_max|trans_br_r_mean      |trans_br_x_mean    |trans_rate_a_sum |tap_mean          |total_cost        |\n",
      "+--------------------------------------------------------------------------------------------------------------+--------------+------------------+------+------------------+-----------------+------------------+---------+------------------+--------------------+--------------------+------------------+----------+----------+---------------------+-------------------+-----------------+------------------+------------------+\n",
      "|gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15009.json|171           |154.14844499999998|1.0   |101.79871345029237|2637.030994152047|-4.104877192982458|281      |179.08926586405988|0.028101742751865683|0.028101742751865683|3.3398004439999984|0.00100208|999.99    |0.0035312410192708294|0.12213300296875002|846.2510999999997|1.0187825520833336|461563.3224267579 |\n",
      "|gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15000.json|171           |154.14844499999998|1.0   |101.79871345029237|2637.030994152047|-4.104877192982458|281      |179.44428559125154|0.028101742751865683|0.028101742751865683|3.3398004439999984|0.00100208|999.99    |0.0035312410192708294|0.12213300296875002|846.2510999999997|1.0187825520833336|463171.39016584953|\n",
      "|gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15030.json|171           |154.14844499999998|1.0   |101.79871345029237|2637.030994152047|-4.104877192982458|281      |176.5309867083523 |0.028101742751865683|0.028101742751865683|3.3398004439999984|0.00100208|999.99    |0.0035312410192708294|0.12213300296875002|846.2510999999997|1.0187825520833336|449264.45468316565|\n",
      "|gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15040.json|171           |154.14844499999998|1.0   |101.79871345029237|2637.030994152047|-4.104877192982458|281      |176.31096444945032|0.028101742751865683|0.028101742751865683|3.3398004439999984|0.00100208|999.99    |0.0035312410192708294|0.12213300296875002|846.2510999999997|1.0187825520833336|448213.1858805311 |\n",
      "|gs://bucketseattledonna/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15066.json|171           |154.14844499999998|1.0   |101.79871345029237|2637.030994152047|-4.104877192982458|281      |179.69479932052823|0.028101742751865683|0.028101742751865683|3.3398004439999984|0.00100208|999.99    |0.0035312410192708294|0.12213300296875002|846.2510999999997|1.0187825520833336|464302.71307693457|\n",
      "+--------------------------------------------------------------------------------------------------------------+--------------+------------------+------+------------------+-----------------+------------------+---------+------------------+--------------------+--------------------+------------------+----------+----------+---------------------+-------------------+-----------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "All filenames are unique\n"
     ]
    }
   ],
   "source": [
    "# Use inner joins on 'filename' (the snapshot key) to combine all real values\n",
    "agg_df = gen_agg.join(load_agg, \"filename\", \"inner\") \\\n",
    "                .join(ac_line_agg, \"filename\", \"inner\") \\\n",
    "                .join(trans_agg, \"filename\", \"inner\") \\\n",
    "                .join(obj_df, \"filename\", \"inner\")\n",
    "\n",
    "# Validate the result\n",
    "print(\"Total number of rows after aggregation:\", agg_df.count())\n",
    "agg_df.show(5, truncate=False)  # Display the first 5 rows without truncation\n",
    "\n",
    "# Check for duplicate filenames\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "duplicate_check = agg_df.groupBy(\"filename\").agg(\n",
    "    F.count(\"*\").alias(\"count\")\n",
    ").filter(F.col(\"count\") > 1)\n",
    "\n",
    "if duplicate_check.count() == 0:\n",
    "    print(\"All filenames are unique\")\n",
    "else:\n",
    "    print(\"Duplicate filenames exist! Data consistency needs to be checked\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address missing data\n",
    "Author: Ewan Long\n",
    "\n",
    "Date: 03/24/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, I will do the data quality checks, and manages any potential missing data, and enforces consistency for a prepared aggregated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, Calculates the count and proportion of missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "missing_count_df = agg_df.select([\n",
    "    F.count(F.when(F.isnull(c), c)).alias(c) for c in agg_df.columns\n",
    "])\n",
    "\n",
    "# Total rows in DataFrame\n",
    "total_rows = agg_df.count()\n",
    "total_row_df = spark.createDataFrame([(total_rows,)], [\"total_rows\"])\n",
    "\n",
    "# Combine counts and total row number via cross join\n",
    "missing_stats = missing_count_df.crossJoin(total_row_df)\n",
    "\n",
    "# Compute missing ratios\n",
    "missing_stats = missing_stats.select(\n",
    "    # Original missing counts\n",
    "    *[F.col(c) for c in agg_df.columns],\n",
    "    # Missing ratio per column\n",
    "    *[(F.col(c) / F.col(\"total_rows\")).alias(f\"{c}_missing_ratio\") for c in agg_df.columns]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the overview of missing values and their ratios for each column in the aggregated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 100\n",
      "-RECORD 0-----------------------------\n",
      " filename                       | 0   \n",
      " num_generators                 | 0   \n",
      " total_pg                       | 0   \n",
      " avg_vg                         | 0   \n",
      " avg_cost_squared               | 0   \n",
      " avg_cost_linear                | 0   \n",
      " avg_cost_offset                | 0   \n",
      " num_loads                      | 0   \n",
      " total_pd                       | 0   \n",
      " br_r_mean                      | 0   \n",
      " br_x_mean                      | 0   \n",
      " rate_a_sum                     | 0   \n",
      " rate_b_min                     | 0   \n",
      " rate_c_max                     | 0   \n",
      " trans_br_r_mean                | 0   \n",
      " trans_br_x_mean                | 0   \n",
      " trans_rate_a_sum               | 0   \n",
      " tap_mean                       | 0   \n",
      " total_cost                     | 0   \n",
      " filename_missing_ratio         | 0.0 \n",
      " num_generators_missing_ratio   | 0.0 \n",
      " total_pg_missing_ratio         | 0.0 \n",
      " avg_vg_missing_ratio           | 0.0 \n",
      " avg_cost_squared_missing_ratio | 0.0 \n",
      " avg_cost_linear_missing_ratio  | 0.0 \n",
      " avg_cost_offset_missing_ratio  | 0.0 \n",
      " num_loads_missing_ratio        | 0.0 \n",
      " total_pd_missing_ratio         | 0.0 \n",
      " br_r_mean_missing_ratio        | 0.0 \n",
      " br_x_mean_missing_ratio        | 0.0 \n",
      " rate_a_sum_missing_ratio       | 0.0 \n",
      " rate_b_min_missing_ratio       | 0.0 \n",
      " rate_c_max_missing_ratio       | 0.0 \n",
      " trans_br_r_mean_missing_ratio  | 0.0 \n",
      " trans_br_x_mean_missing_ratio  | 0.0 \n",
      " trans_rate_a_sum_missing_ratio | 0.0 \n",
      " tap_mean_missing_ratio         | 0.0 \n",
      " total_cost_missing_ratio       | 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display missing value statistics clearly\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "missing_stats.show(vertical=True, truncate=False)  \n",
    "# using vertical=True improves readability for large number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Pipelines and Doing Experiments\n",
    "  \n",
    "Author: Donna Nguyen\n",
    "  \n",
    "Date: 03/24/25\n",
    "  \n",
    "Create baseline pipelines and do experiments on your data:\n",
    "\n",
    "Set a blind test set that is never seen during training (10% of data)\n",
    "Report evaluation metrics over the training set\n",
    "Report evaluation metrics over blind test dataset\n",
    "Create a baseline model using linear regression\n",
    "Discussion of experimental results on training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set -> RMSE: 270.7873, R: 0.9983\n",
      "Test Set -> RMSE: 521.4989, R: 0.9969\n",
      "+------------------+-----------------+\n",
      "|total_cost        |prediction       |\n",
      "+------------------+-----------------+\n",
      "|454043.8197398794 |454131.215759768 |\n",
      "|452002.79294327396|452704.7248362681|\n",
      "|444792.7982146128 |444610.5818190558|\n",
      "|455445.203582367  |455317.9064252551|\n",
      "|436232.22792202106|435323.1401192636|\n",
      "+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define numerical columns (exclude total_cost and filename)\n",
    "numerical_columns = [col for col in agg_df.columns if col not in [\"total_cost\", \"filename\"]]\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=numerical_columns, outputCol=\"unscaled_features\")\n",
    "\n",
    "# Apply MinMaxScaler for feature scaling\n",
    "scaler = MinMaxScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
    "\n",
    "# Define Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_cost\", maxIter=100, regParam=0.1)\n",
    "\n",
    "# Create ML pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "# Split data into training (90%) and blind test set (10%)\n",
    "train_data, test_data = agg_df.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on train and test sets\n",
    "train_predictions = model.transform(train_data)\n",
    "test_predictions = model.transform(test_data)\n",
    "\n",
    "# Define evaluator for RMSE and R\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"total_cost\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"total_cost\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Compute RMSE and R for training set\n",
    "train_rmse = rmse_evaluator.evaluate(train_predictions)\n",
    "train_r2 = r2_evaluator.evaluate(train_predictions)\n",
    "\n",
    "# Compute RMSE and R for test set\n",
    "test_rmse = rmse_evaluator.evaluate(test_predictions)\n",
    "test_r2 = r2_evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Print model performance\n",
    "print(f\"Training Set -> RMSE: {train_rmse:.4f}, R: {train_r2:.4f}\")\n",
    "print(f\"Test Set -> RMSE: {test_rmse:.4f}, R: {test_r2:.4f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "test_predictions.select(\"total_cost\", \"prediction\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set -> RMSE: 270.7873, MSE: 73325.7589, R: 0.9983\n",
      "Test Set -> RMSE: 521.4989, MSE: 271961.0970, R: 0.9969\n"
     ]
    }
   ],
   "source": [
    "# Define evaluators for RMSE, R, and MSE\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"total_cost\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"total_cost\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "mse_evaluator = RegressionEvaluator(labelCol=\"total_cost\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "\n",
    "# Compute metrics for training set\n",
    "train_rmse = rmse_evaluator.evaluate(train_predictions)\n",
    "train_r2 = r2_evaluator.evaluate(train_predictions)\n",
    "train_mse = mse_evaluator.evaluate(train_predictions)\n",
    "\n",
    "# Compute metrics for test set\n",
    "test_rmse = rmse_evaluator.evaluate(test_predictions)\n",
    "test_r2 = r2_evaluator.evaluate(test_predictions)\n",
    "test_mse = mse_evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Print model performance\n",
    "print(f\"Training Set -> RMSE: {train_rmse:.4f}, MSE: {train_mse:.4f}, R: {train_r2:.4f}\")\n",
    "print(f\"Test Set -> RMSE: {test_rmse:.4f}, MSE: {test_mse:.4f}, R: {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussions\n",
    "\n",
    "The baseline linear regression model demonstrated strong predictive performance on both the training and test datasets. A high R (close to 1) indicate that the variance in total_cost is well-explained by the provided quantitative features. The test RMSE is higher than the training RMSE, indicating a slight increase in prediction error on unseen data. However, the high R value (0.9969) suggests that the model still generalizes well. The prediction values align quite closely with the actual cost so far. But this is only an experiment on a small sample though. The slight performance drop from training to test set suggests some degree of overfitting, though the effect seems minimal. Further improvement could involve regularization tuning (adjusting regParam), more Feature engineering or just trying other models for comparison.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
