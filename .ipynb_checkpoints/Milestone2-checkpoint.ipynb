{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS5460 Milestone 2 - EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting Files to PySpark\n",
    "\n",
    "Author: Anne Tumlin\n",
    "\n",
    "Date: 03/21/25\n",
    "\n",
    "Now that we have taken the files from the original GCS bucket, extracted them, and put them in our local GCS bucket (see `docs/EXTRACTION_PROCESS` in GitHub for more details), we can begin to ingest our data into PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/28 20:42:29 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/28 20:42:29 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/28 20:42:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/28 20:42:29 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, explode, input_file_name, expr, sum as spark_sum, avg, count\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"app_name\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to edit the path with YOUR google storage bucket here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/*.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on a Small Scale\n",
    "Let's try this with only 100 files first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: Change bucket_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucket_name = \"ds5460-tumlinam-fp-bucket\"\n",
    "bucket_name = \"bucketnashvilledonna\"\n",
    "bucket = client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List blobs (files) in the specified prefix and collect the first 100 file paths\n",
    "blobs = bucket.list_blobs(prefix=prefix)\n",
    "file_paths = []\n",
    "for blob in blobs:\n",
    "    file_paths.append(f\"gs://{bucket_name}/{blob.name}\")\n",
    "    if len(file_paths) >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** After testing and running into issues with the schema, I discovered that due to the way the JSON files are formatted we must utilize the multiline read option. Otherwise, our data will not be read in properly. Instead, it will lead to the error `|-- _corrupt_record: string (nullable = true)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, read all JSON files with the multiline option enabled so that nested JSON is parsed correctly. Adding the original **filename** column for identification and join purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading took 10.11 seconds\n",
      "Initial partitions: 4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_small = spark.read.option(\"multiline\", \"true\").json(file_paths).withColumn(\"filename\", input_file_name())\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Reading took {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Initial partitions: {df_small.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- grid: struct (nullable = true)\n",
      " |    |-- context: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |-- edges: struct (nullable = true)\n",
      " |    |    |-- ac_line: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- generator_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- load_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- shunt_link: struct (nullable = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- transformer: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |-- nodes: struct (nullable = true)\n",
      " |    |    |-- bus: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- generator: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- load: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- shunt: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- objective: double (nullable = true)\n",
      " |-- solution: struct (nullable = true)\n",
      " |    |-- edges: struct (nullable = true)\n",
      " |    |    |-- ac_line: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- transformer: struct (nullable = true)\n",
      " |    |    |    |-- features: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- receivers: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- senders: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |-- nodes: struct (nullable = true)\n",
      " |    |    |-- bus: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- generator: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |-- filename: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema to verify that 'grid.nodes.generator' and others are parsed correctly.\n",
    "df_small.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data Explanation:***\n",
    "The above Schema provides a high-level overview of the OPFData dataset. The dataset is structured at the root into three main components: grid, solution, and metadata. The grid represents the system's state before optimization, serving as the input to the OPF solver. This represents a snapshot of the grid state. So, the grid contains various generators, loads, and transmission lines with various physical measurements as features. The initial state of the grid is stored within the grid. The solution contains the results of the OPF computation, representing the optimized grid state by taking the input grid features, solving multiple complex optimization equations, and outputting the optimal power generation levels. \n",
    "\n",
    "Finally, the metadata includes the objective, corresponding to the total generation cost, which is based on the optimal power generation level that is needed to optimize the grid (the OPF task). We will focus on the grid data and metadata for this project, as these contain the necessary inputs and cost information without revealing the OPF solution itself. Excluding the solution data ensures that our machine learning model learns to predict the objective cost solely based on system constraints rather than indirectly relying on solved OPF values. This approach prevents data leakage and maintains the integrity of our predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                grid|            metadata|            solution|            filename|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|[[[[100.0]]], [[[...| [443934.8106702195]|[[[[[1.2271252469...|gs://ds5460-tumli...|\n",
      "|[[[[100.0]]], [[[...|[465533.45792886155]|[[[[[1.3700529900...|gs://ds5460-tumli...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading All Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading took 848.17 seconds\n",
      "Initial partitions: 556\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df = spark.read.option(\"multiline\", \"true\").json(json_path).withColumn(\"filename\", input_file_name())\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Reading took {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Initial partitions: {df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it takes quite a while for PySpark to read in the 15,000 JSON files included in our dataset. Therefore, we do not want to do this repeatedly when working in a new session. Optimization of this process can be seen below after dataframe aggregation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Summarizing Relevant Datasets Code for the New Dataset\n",
    "  \n",
    "Author: Ewan Long\n",
    "  \n",
    "Date: 03/23/25\n",
    "\n",
    "Code edits and some explanations added by: Anne Tumlin, Ewan Long\n",
    "\n",
    "Edit date: 03/25/25, 03/28/25\n",
    "  \n",
    "We are trying to test and revise the original code we have in the Milestone 1 as we changed our dataset (see `docs/EXTRACTION_PROCESS` in GitHub for reasons & more details), we have ingested our data into PySpark. \n",
    "  \n",
    "The following code will try to aggregate data and join the key tables (see Milestone 1 Notebook and doc for reasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the data processing. Let's process generator data first. \n",
    "  \n",
    "I will explode nested JSON and extract generator attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+--------------------+-----+--------+--------+-------+---+------------+-----------+-----------+\n",
      "|            filename|mbase|                 pg|                pmin| pmax|      qg|    qmin|   qmax| vg|cost_squared|cost_linear|cost_offset|\n",
      "+--------------------+-----+-------------------+--------------------+-----+--------+--------+-------+---+------------+-----------+-----------+\n",
      "|gs://ds5460-tumli...| 46.9|            0.34463|             0.22026|0.469| 0.07856|-0.02298| 0.1801|1.0|         0.0|     3000.0|        0.0|\n",
      "|gs://ds5460-tumli...|888.2|            6.43217|  3.9823399999999998|8.882| 1.27013|-0.72832|3.26858|1.0|       10.98|      910.9|        0.0|\n",
      "|gs://ds5460-tumli...| 25.0|            0.16691| 0.08381999999999999| 0.25|0.041875|-0.01225|  0.096|1.0|         0.0|     3000.0|        0.0|\n",
      "|gs://ds5460-tumli...| 25.0|            0.13858|0.027160000000000004| 0.25|0.041875|-0.01225|  0.096|1.0|         0.0|     3000.0|        0.0|\n",
      "|gs://ds5460-tumli...| 25.0|0.17330500000000001|             0.09661| 0.25|0.041875|-0.01225|  0.096|1.0|         0.0|     3000.0|        0.0|\n",
      "+--------------------+-----+-------------------+--------------------+-----+--------+--------+-------+---+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_df = df.select(\n",
    "    \"filename\",\n",
    "    explode(\"grid.nodes.generator\").alias(\"generator_array\")\n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"generator_array\")[0].alias(\"mbase\"),\n",
    "    col(\"generator_array\")[1].alias(\"pg\"),\n",
    "    col(\"generator_array\")[2].alias(\"pmin\"),\n",
    "    col(\"generator_array\")[3].alias(\"pmax\"),\n",
    "    col(\"generator_array\")[4].alias(\"qg\"),\n",
    "    col(\"generator_array\")[5].alias(\"qmin\"),\n",
    "    col(\"generator_array\")[6].alias(\"qmax\"),\n",
    "    col(\"generator_array\")[7].alias(\"vg\"),\n",
    "    col(\"generator_array\")[8].alias(\"cost_squared\"),\n",
    "    col(\"generator_array\")[9].alias(\"cost_linear\"),\n",
    "    col(\"generator_array\")[10].alias(\"cost_offset\")\n",
    ")\n",
    "\n",
    "gen_df.show(5) # verify the extracted attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to aggregates generator features at the snapshort level, summarizing key characteristics for each filename.\n",
    "  \n",
    "The following statistics will be extracted and computed:\n",
    "- Total number of generators\n",
    "- Sum of generated power (pg)\n",
    "- Average generator voltage (vg)\n",
    "- Average generator cost parameters (quadratic, linear, and offset terms)\n",
    "\n",
    "Reactive power features (qg, qmin, qmax, qd) are disregarded. This is due to the fact that these features primarily influence grid stability which is used in OPF calculations but does not have as much effect on cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate generator features by snapshot (filename)\n",
    "gen_agg = gen_df.groupBy(\"filename\").agg(\n",
    "    count(\"*\").alias(\"num_generators\"),\n",
    "    spark_sum(\"pg\").alias(\"total_pg\"),\n",
    "    avg(\"vg\").alias(\"avg_vg\"),\n",
    "    avg(\"cost_squared\").alias(\"avg_cost_squared\"),\n",
    "    avg(\"cost_linear\").alias(\"avg_cost_linear\"),\n",
    "    avg(\"cost_offset\").alias(\"avg_cost_offset\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would try to process and aggregate the power load data, extracting key load parameters (specifically real \"pd\" and reactive \"qd\" power demands) per snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+\n",
      "|            filename|                 pd|                 qd|\n",
      "+--------------------+-------------------+-------------------+\n",
      "|gs://ds5460-tumli...|0.39334149858902057|0.07845096459690429|\n",
      "|gs://ds5460-tumli...| 0.8026295510428011| 0.2988552641010892|\n",
      "|gs://ds5460-tumli...| 0.4029364561377476|0.10125973116109559|\n",
      "|gs://ds5460-tumli...| 0.6994859094100779|0.03729018007517079|\n",
      "|gs://ds5460-tumli...| 0.9347066102518853| 0.1550109690774743|\n",
      "+--------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting load data\n",
    "load_df = df.select(\n",
    "    \"filename\",\n",
    "    col(\"grid.nodes.load\").alias(\"load_arrays\")  \n",
    ").select(\n",
    "    \"filename\",\n",
    "    explode(\"load_arrays\").alias(\"load\")  \n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"load\")[0].alias(\"pd\"),  \n",
    "    col(\"load\")[1].alias(\"qd\")\n",
    ")\n",
    "\n",
    "load_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate load features per snapshot\n",
    "load_agg = load_df.groupBy(\"filename\").agg(\n",
    "    count(\"*\").alias(\"num_loads\"),\n",
    "    spark_sum(\"pd\").alias(\"total_pd\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the next step would try to extract and aggregate transmission line features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+---------+------------------+\n",
      "|            filename|      br_r|      br_x|    rate_a|   rate_b|            rate_c|\n",
      "+--------------------+----------+----------+----------+---------+------------------+\n",
      "|gs://ds5460-tumli...|0.01340085|0.01340085| 0.0154525|0.0792528|            2.3994|\n",
      "|gs://ds5460-tumli...| 0.0065814| 0.0065814|0.00358046|0.0258456|            2.6003|\n",
      "|gs://ds5460-tumli...| 0.0065814| 0.0065814|0.00358046|0.0258456|            2.6003|\n",
      "|gs://ds5460-tumli...| 0.0062579| 0.0062579|0.00319977|0.0219502|2.5658999999999996|\n",
      "|gs://ds5460-tumli...|0.00557485|0.00557485|0.00696197|0.0360534|            2.1877|\n",
      "+--------------------+----------+----------+----------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract transmission line features from grid.edges.ac_line.features\n",
    "ac_line_df = df.select(\n",
    "    \"filename\",\n",
    "    explode(\"grid.edges.ac_line.features\").alias(\"ac_line\")\n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"ac_line\")[2].alias(\"br_r\"),\n",
    "    col(\"ac_line\")[3].alias(\"br_x\"),\n",
    "    col(\"ac_line\")[4].alias(\"rate_a\"),\n",
    "    col(\"ac_line\")[5].alias(\"rate_b\"),\n",
    "    col(\"ac_line\")[6].alias(\"rate_c\")\n",
    ")\n",
    "\n",
    "ac_line_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate transformer features per snapshot\n",
    "ac_line_agg = ac_line_df.groupBy(\"filename\").agg(\n",
    "    avg(\"br_r\").alias(\"br_r_mean\"),\n",
    "    avg(\"br_x\").alias(\"br_x_mean\"),\n",
    "    spark_sum(\"rate_a\").alias(\"rate_a_sum\"),\n",
    "    F.min(\"rate_b\").alias(\"rate_b_min\"),\n",
    "    F.max(\"rate_c\").alias(\"rate_c_max\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, extract and aggregate the Transformer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+------+-------+\n",
      "|            filename|      br_r|     br_x|rate_a|    tap|\n",
      "+--------------------+----------+---------+------+-------+\n",
      "|gs://ds5460-tumli...|5.36254E-4|0.0293108| 3.905| 1.0125|\n",
      "|gs://ds5460-tumli...|5.05881E-4|0.0243569|5.0692|1.03125|\n",
      "|gs://ds5460-tumli...|5.41271E-4|0.0336934|4.6108|    1.1|\n",
      "|gs://ds5460-tumli...|5.41271E-4|0.0336934|4.6108|    1.1|\n",
      "|gs://ds5460-tumli...|5.41271E-4|0.0336934|4.6108|    1.1|\n",
      "+--------------------+----------+---------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract transformer features from grid.edges.transformer.features\n",
    "trans_df = df.select(\n",
    "    \"filename\",\n",
    "    explode(\"grid.edges.transformer.features\").alias(\"trans\")\n",
    ").select(\n",
    "    \"filename\",\n",
    "    col(\"trans\")[2].alias(\"br_r\"),\n",
    "    col(\"trans\")[3].alias(\"br_x\"),\n",
    "    col(\"trans\")[4].alias(\"rate_a\"),\n",
    "    col(\"trans\")[7].alias(\"tap\")\n",
    ")\n",
    "\n",
    "trans_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate transformer features per snapshot\n",
    "trans_agg = trans_df.groupBy(\"filename\").agg(\n",
    "    avg(\"br_r\").alias(\"trans_br_r_mean\"),\n",
    "    avg(\"br_x\").alias(\"trans_br_x_mean\"),\n",
    "    spark_sum(\"rate_a\").alias(\"trans_rate_a_sum\"),\n",
    "    avg(\"tap\").alias(\"tap_mean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, extract the objective value (total cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            filename|        total_cost|\n",
      "+--------------------+------------------+\n",
      "|gs://ds5460-tumli...| 450456.5324264121|\n",
      "|gs://ds5460-tumli...| 449445.5757519682|\n",
      "|gs://ds5460-tumli...| 439303.5076971049|\n",
      "|gs://ds5460-tumli...|458191.26107565826|\n",
      "|gs://ds5460-tumli...| 452500.4108687909|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the objective value directly from metadata\n",
    "obj_df = df.select(\n",
    "    \"filename\",\n",
    "    col(\"metadata.`objective`\").alias(\"total_cost\") \n",
    ")\n",
    "\n",
    "\n",
    "obj_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will merge aggregated data from different sources and ensures data consistency by checking for duplicate entries. Here are the main steps:\n",
    "  \n",
    "- Joining aggregated data from generator, load, transmission line, transformer and objective value based on \"filename\" into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows after aggregation: 15000\n"
     ]
    }
   ],
   "source": [
    "# Use inner joins on 'filename' (the snapshot key) to combine all real values\n",
    "agg_df = gen_agg.join(load_agg, \"filename\", \"inner\") \\\n",
    "                .join(ac_line_agg, \"filename\", \"inner\") \\\n",
    "                .join(trans_agg, \"filename\", \"inner\") \\\n",
    "                .join(obj_df, \"filename\", \"inner\")\n",
    "\n",
    "# Validate the result\n",
    "print(\"Total number of rows after aggregation:\", agg_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes quite a while to run. We do NOT want to do this everytime we run our code. Therefore, we will optimize this process by saving this dataframe as a parquet file which we will read into PySpark in future coding sessions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.write.mode(\"overwrite\").parquet(\"gs://ds5460-tumlinam-fp-bucket/processed_data/agg_df.parquet\") # Added by Anne "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our optimized dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are directly working with saved parquet file without running the whole process above, you will need to import** `time` **here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading took 5.84 seconds\n"
     ]
    }
   ],
   "source": [
    "# Added by Anne \n",
    "start_time = time.time()\n",
    "\n",
    "agg_df = spark.read.parquet(\"gs://dataproc-temp-us-east1-299400297029-hnyjjiwf/processed_data/agg_df.parquet\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Reading took {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much shorter reading time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's\n",
    "\n",
    "- Validate the aggregated results\n",
    "- Check for duplicate filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+--------------+------------------+------+------------------+-----------------+------------------+---------+------------------+--------------------+--------------------+------------------+----------+----------+---------------------+-------------------+-----------------+------------------+------------------+\n",
      "|filename                                                                                                             |num_generators|total_pg          |avg_vg|avg_cost_squared  |avg_cost_linear  |avg_cost_offset   |num_loads|total_pd          |br_r_mean           |br_x_mean           |rate_a_sum        |rate_b_min|rate_c_max|trans_br_r_mean      |trans_br_x_mean    |trans_rate_a_sum |tap_mean          |total_cost        |\n",
      "+---------------------------------------------------------------------------------------------------------------------+--------------+------------------+------+------------------+-----------------+------------------+---------+------------------+--------------------+--------------------+------------------+----------+----------+---------------------+-------------------+-----------------+------------------+------------------+\n",
      "|gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15112.json|171           |154.14844499999998|1.0   |101.79871345029237|2637.030994152047|-4.104877192982458|281      |178.07434280491296|0.028101742751865683|0.028101742751865683|3.3398004439999984|0.00100208|999.99    |0.0035312410192708294|0.12213300296875002|846.2510999999997|1.0187825520833336|457017.6915453811 |\n",
      "|gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15156.json|171           |154.14844499999998|1.0   |101.79871345029237|2637.030994152047|-4.104877192982458|281      |177.75404115936618|0.028101742751865683|0.028101742751865683|3.3398004439999984|0.00100208|999.99    |0.0035312410192708294|0.12213300296875002|846.2510999999997|1.0187825520833336|454991.87287162687|\n",
      "|gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15159.json|171           |154.14844499999998|1.0   |101.79871345029237|2637.030994152047|-4.104877192982458|281      |176.44749746510547|0.028101742751865683|0.028101742751865683|3.3398004439999984|0.00100208|999.99    |0.0035312410192708294|0.12213300296875002|846.2510999999997|1.0187825520833336|448851.02127889916|\n",
      "|gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15652.json|171           |154.14844499999998|1.0   |101.79871345029237|2637.030994152047|-4.104877192982458|281      |179.48218836865323|0.028101742751865683|0.028101742751865683|3.3398004439999984|0.00100208|999.99    |0.0035312410192708294|0.12213300296875002|846.2510999999997|1.0187825520833336|463761.24465372495|\n",
      "|gs://ds5460-tumlinam-fp-bucket/gridopt-dataset-tmp/dataset_release_1/pglib_opf_case500_goc/group_1/example_15806.json|171           |154.14844499999998|1.0   |101.79871345029237|2637.030994152047|-4.104877192982458|281      |174.48837846307646|0.028101742751865683|0.028101742751865683|3.3398004439999984|0.00100208|999.99    |0.0035312410192708294|0.12213300296875002|846.2510999999997|1.0187825520833336|440065.6724156548 |\n",
      "+---------------------------------------------------------------------------------------------------------------------+--------------+------------------+------+------------------+-----------------+------------------+---------+------------------+--------------------+--------------------+------------------+----------+----------+---------------------+-------------------+-----------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df.show(5, truncate=False)  # Display the first 5 rows without truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All filenames are unique\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for duplicate filenames\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "duplicate_check = agg_df.groupBy(\"filename\").agg(\n",
    "    F.count(\"*\").alias(\"count\")\n",
    ").filter(F.col(\"count\") > 1)\n",
    "\n",
    "if duplicate_check.count() == 0:\n",
    "    print(\"All filenames are unique\")\n",
    "else:\n",
    "    print(\"Duplicate filenames exist! Data consistency needs to be checked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check & Address Missing Data\n",
    "Author: Ewan Long\n",
    "\n",
    "Date: 03/24/25\n",
    "\n",
    "Edit Date: 03/27/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, I will do the data quality checks, and manages any potential missing data, and enforces consistency for a prepared aggregated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, Calculates the count and proportion of missing values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count missing values per column\n",
    "missing_count_df = agg_df.select([\n",
    "    F.count(F.when(F.isnull(c), c)).alias(c) for c in agg_df.columns\n",
    "])\n",
    "\n",
    "# Total rows in DataFrame\n",
    "total_rows = agg_df.count()\n",
    "total_row_df = spark.createDataFrame([(total_rows,)], [\"total_rows\"])\n",
    "\n",
    "# Combine counts and total row number via cross join\n",
    "missing_stats = missing_count_df.crossJoin(total_row_df)\n",
    "\n",
    "# Compute missing ratios\n",
    "missing_stats = missing_stats.select(\n",
    "    # Original missing counts\n",
    "    *[F.col(c) for c in agg_df.columns],\n",
    "    # Missing ratio per column\n",
    "    *[(F.col(c) / F.col(\"total_rows\")).alias(f\"{c}_missing_ratio\") for c in agg_df.columns]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the overview of missing values and their ratios for each column in the aggregated DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/28 20:44:27 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " filename                       | 0   \n",
      " num_generators                 | 0   \n",
      " total_pg                       | 0   \n",
      " avg_vg                         | 0   \n",
      " avg_cost_squared               | 0   \n",
      " avg_cost_linear                | 0   \n",
      " avg_cost_offset                | 0   \n",
      " num_loads                      | 0   \n",
      " total_pd                       | 0   \n",
      " br_r_mean                      | 0   \n",
      " br_x_mean                      | 0   \n",
      " rate_a_sum                     | 0   \n",
      " rate_b_min                     | 0   \n",
      " rate_c_max                     | 0   \n",
      " trans_br_r_mean                | 0   \n",
      " trans_br_x_mean                | 0   \n",
      " trans_rate_a_sum               | 0   \n",
      " tap_mean                       | 0   \n",
      " total_cost                     | 0   \n",
      " filename_missing_ratio         | 0.0 \n",
      " num_generators_missing_ratio   | 0.0 \n",
      " total_pg_missing_ratio         | 0.0 \n",
      " avg_vg_missing_ratio           | 0.0 \n",
      " avg_cost_squared_missing_ratio | 0.0 \n",
      " avg_cost_linear_missing_ratio  | 0.0 \n",
      " avg_cost_offset_missing_ratio  | 0.0 \n",
      " num_loads_missing_ratio        | 0.0 \n",
      " total_pd_missing_ratio         | 0.0 \n",
      " br_r_mean_missing_ratio        | 0.0 \n",
      " br_x_mean_missing_ratio        | 0.0 \n",
      " rate_a_sum_missing_ratio       | 0.0 \n",
      " rate_b_min_missing_ratio       | 0.0 \n",
      " rate_c_max_missing_ratio       | 0.0 \n",
      " trans_br_r_mean_missing_ratio  | 0.0 \n",
      " trans_br_x_mean_missing_ratio  | 0.0 \n",
      " trans_rate_a_sum_missing_ratio | 0.0 \n",
      " tap_mean_missing_ratio         | 0.0 \n",
      " total_cost_missing_ratio       | 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display missing value statistics clearly\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "missing_stats.show(vertical=True, truncate=False)  \n",
    "# using vertical=True improves readability for large number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "Based on the output, we have no missing values in the DataFrame, so no data cleaning action is needed.\n",
    "  \n",
    "As our data contains only numerical features, so no extra addressing non-numerical features action is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Pipelines and Doing Experiments\n",
    "  \n",
    "Author: Donna Nguyen\n",
    "  \n",
    "Date: 03/24/25\n",
    "  \n",
    "Once we have leveraged the whole dataset, we now create baseline pipelines and do experiments on the data by following the below steps:\n",
    "\n",
    "Set a blind test set that is never seen during training (10% of data). \n",
    "\n",
    "Report evaluation metrics over the training set. \n",
    "\n",
    "Report evaluation metrics over blind test dataset.\n",
    "\n",
    "Create a baseline model using linear regression (Note that we do not have non-numerical features). \n",
    "\n",
    "Discussion of experimental results on training and testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 35.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define numerical columns (exclude total_cost and filename)\n",
    "numerical_columns = [col for col in agg_df.columns if col not in [\"total_cost\", \"filename\"]]\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=numerical_columns, outputCol=\"unscaled_features\")\n",
    "\n",
    "# Apply MinMaxScaler for feature scaling\n",
    "scaler = MinMaxScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
    "\n",
    "# Define Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_cost\", maxIter=100, regParam=0.1)\n",
    "\n",
    "# Create ML pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "# Split data into training (90%) and blind test set (10%)\n",
    "train_data, test_data = agg_df.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training took {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on train and test sets\n",
    "train_predictions = model.transform(train_data)\n",
    "test_predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|total_cost        |prediction        |\n",
      "+------------------+------------------+\n",
      "|436232.22792202106|435242.6040623386 |\n",
      "|448971.5047527308 |449420.8345476628 |\n",
      "|456951.99127336533|456861.6436028836 |\n",
      "|459014.66775759   |459587.36927049747|\n",
      "|458907.8245283152 |458687.01120999246|\n",
      "+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show sample predictions\n",
    "test_predictions.select(\"total_cost\", \"prediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set -> RMSE: 297.2126, MSE: 88335.3203, R²: 0.9981\n",
      "Test Set -> RMSE: 303.2414, MSE: 91955.3663, R²: 0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define evaluators for RMSE, R², and MSE\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"total_cost\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"total_cost\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "mse_evaluator = RegressionEvaluator(labelCol=\"total_cost\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "\n",
    "# Compute metrics for training set\n",
    "train_rmse = rmse_evaluator.evaluate(train_predictions)\n",
    "train_r2 = r2_evaluator.evaluate(train_predictions)\n",
    "train_mse = mse_evaluator.evaluate(train_predictions)\n",
    "\n",
    "# Compute metrics for test set\n",
    "test_rmse = rmse_evaluator.evaluate(test_predictions)\n",
    "test_r2 = r2_evaluator.evaluate(test_predictions)\n",
    "test_mse = mse_evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Print model performance\n",
    "print(f\"Training Set -> RMSE: {train_rmse:.4f}, MSE: {train_mse:.4f}, R²: {train_r2:.4f}\")\n",
    "print(f\"Test Set -> RMSE: {test_rmse:.4f}, MSE: {test_mse:.4f}, R²: {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussions\n",
    "\n",
    "The baseline linear regression model demonstrated strong predictive performance on both the training and test datasets. A high R² in both sets (almost close to 1) indicate that the variance in total_cost is well-explained by the provided quantitative features. The test RMSE is higher than the training RMSE, indicating a slight increase in prediction error on unseen data. However, the high R² value (0.9981) suggests that the model still generalizes well, whether in training or test set. The prediction values align quite closely with the actual cost. We have leveraged the whole dataset and this has shown to be very good results even with just a simple model. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
